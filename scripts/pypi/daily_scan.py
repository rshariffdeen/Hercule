import time
from bs4 import BeautifulSoup
import requests
import feedparser
import os
import json
import ssl
from typing import Any

from scripts.experiments.violin_plot import file_name

ssl._create_default_https_context = ssl._create_unverified_context
PKG_LIST_FILE = "packages.txt"
RSS_FEED = "https://pypi.org/rss/packages.xml"
DATABASE_FILE = "pypi_packages.json"


def write_as_json(data: Any, output_file_path: str):
    content = json.dumps(data)
    with open(output_file_path, "w") as out_file:
        out_file.writelines(content)


def read_json(file_path: str):
    json_data = None
    if os.path.isfile(file_path):
        with open(file_path, "r") as in_file:
            content = in_file.readlines()
            if len(content) == 0:
                content_str = "{}"
            elif len(content) > 1:
                content_str = " ".join([l.strip().replace("\n", "") for l in content])
            else:
                content_str = content[0]
            json_data = json.loads(content_str)
    return json_data

def get_html(url):
    page = requests.get(url)
    soup = BeautifulSoup(page.content, "html.parser", from_encoding="iso-8859-1")
    return soup


def get_pkg_list(html_soup):
    pkg_list = []
    anchor_list = html_soup.find_all("a")
    for anchor in anchor_list:
        pkg_name = anchor.contents[0]
        pkg_list.append(pkg_name)
    return pkg_list


def get_latest_feed():
    latest_feed = feedparser.parse(RSS_FEED)
    entries = latest_feed["entries"]
    print(f"fetched number of entries: {len(entries)}")
    print("reading database")
    database_entries = read_json(DATABASE_FILE)
    print(f"found package count: {len(database_entries)}")
    new_package_list = []
    for entry in entries:
        pub_date = "{0}-{0}-{0}".format(entry.published_parsed.tm_year,
                                        entry.published_parsed.tm_mon,
                                        entry.published_parsed.tm_mday)

        if entry.id not in database_entries:
            database_entries[entry.id] = dict()
            print(f"found NEW package {entry.id} published on {pub_date}")
        else:
            print(f"found updated package {entry.id} published on {pub_date}")
        entry_info = database_entries[entry.id]
        if entry.published not in entry_info:
            entry_info[entry.published] = entry
            new_package_list.append((pub_date, entry.id))
    print("updating database")
    write_as_json(database_entries, DATABASE_FILE)
    print(f"updated package count: {len(database_entries)}")
    return new_package_list

def get_download_links(html_soup):
    download_list = []
    anchor_list = html_soup.find_all("a")
    for anchor in anchor_list:
        download_list.append(anchor["href"])
    return download_list


def download_file(file_url, file_path):
    response = requests.get(file_url)
    if os.path.isfile(file_path):
        print("duplicate file found", file_path)
    dir_path = os.path.dirname(file_path)
    if not os.path.isdir(dir_path):
        os.makedirs(dir_path)
    with open(file_path, 'wb') as f:
        f.write(response.content)
    return


def run():
    while True:
        new_list = get_latest_feed()
        print(f"found {len(new_list)} new updates")
        if not new_list:
            print("sleeping for 3600 seconds")
            time.sleep(3600)
        for update in new_list:
            pub_data, pkg_url = update
            snapshot_dr = f"/data/pypi/sanpshot-{pub_data}"
            if not os.path.isdir(snapshot_dr):
                os.makedirs(snapshot_dr)
            pkg_name = str(pkg_url).replace("https://pypi.org/project/", "").replace("/", "")
            print(f"fetching download links {pkg_name}")
            pypi_url = f"https://pypi.org/simple/{pkg_name.strip()}"
            html_soup = get_html(pypi_url)
            download_list = get_download_links(html_soup)
            print("link list", download_list)
            if not download_list:
                print("empty links", pkg_name)
                continue
            print(f"found {len(download_list)} links", pkg_name)
            latest_link = download_list[-1]
            file_name = str(latest_link).split("#")[0].split("/")[-1]
            download_file(latest_link, f"{snapshot_dr}/{file_name}")
            print(f"downloaded {pkg_name} at {snapshot_dr}/{file_name}")
            print(f"running analysis for {snapshot_dr}/{file_name}")
            os.system(f"/hercule/bin/hercule -F {snapshot_dr}/{file_name}")
            report_json = f"/hercule/results/{file_name}.json"
            json_data = read_json(report_json)
            experiment_dir = f"/hercule/experiments/{file_name}-dir"
            package_results = json_data[experiment_dir]
            has_malicious_deps = package_results["is-compromised"]
            has_malicious_behavior = package_results["has-malicious-behavior"]
            is_malicious = has_malicious_deps or has_malicious_behavior
            if not is_malicious:
                os.system(f"rm {snapshot_dr}/{file_name}")
                os.system(f"rm -rf {experiment_dir}")
            print(f"completed analysis for {snapshot_dr}/{file_name}")


if __name__ == "__main__":
    run()
