import sys

from bs4 import BeautifulSoup
import requests
import os
import ssl
from typing import Any

ssl._create_default_https_context = ssl._create_unverified_context

package_list = ["boto3", "botocore", "certifi", "charset-normalizer",
                "cryptography", "google-api-core", "grpcio-status", "idna",
                "importlib-metadata", "numpy", "packaging", "python-dateutil",
                "pytz", "pyyaml", "requests", "s3transfer", "setuptools", "six",
                "typing-extensions", "urllib3", "wheel"]


def read_file(file_path):
    with open(file_path, "r") as f:
        content = f.readlines()
        return content


def get_html(url):
    page = requests.get(url)
    soup = BeautifulSoup(page.content, "html.parser", from_encoding="iso-8859-1")
    return soup


def get_download_links(html_soup):
    download_list = []
    anchor_list = html_soup.find_all("a")
    for anchor in anchor_list:
        download_list.append(anchor["href"])
    return download_list


def download_file(file_url, file_path):
    response = requests.get(file_url)
    if os.path.isfile(file_path):
        print("duplicate file found", file_path)
    dir_path = os.path.dirname(file_path)
    if not os.path.isdir(dir_path):
        os.makedirs(dir_path)
    with open(file_path, 'wb') as f:
        f.write(response.content)
    return


def run(args):
    global package_list
    if args:
        filepath = args[0]
        package_list = read_file(filepath)

    for pkg in package_list:
        print(pkg)
        pypi_url = f"https://pypi.org/simple/{pkg.strip()}"
        html_soup = get_html(pypi_url)
        download_list = get_download_links(html_soup)
        if not download_list:
            print("empty links", pkg)
            continue
        print("found links", pkg)
        latest_link = download_list[-1]
        file_name = str(latest_link).split("#")[0].split("/")[-1]
        download_file(latest_link, f"/data/benign/{file_name}")


if __name__ == "__main__":
    run(sys.argv[1:])
